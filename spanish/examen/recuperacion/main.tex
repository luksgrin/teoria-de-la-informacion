% !TeX program = xelatex
\documentclass{article}
\usepackage{amsmath}
\usepackage{amsfonts}
\title{\vspace{-6cm}}
\date{}
\setlength{\parindent}{0pt}

\usepackage[a4paper, total={7in, 7in}, bottom=1in, top=2in]{geometry}

\pagenumbering{gobble}
\renewcommand{\labelenumi}{\alph{enumi})}

\begin{document}
\maketitle
Apellidos:\hspace{11cm}Nombre:\\\\
\hspace*{4cm}\textbf{SEGUNDO PARCIAL DE MDTI 2023-2024}\hspace*{3cm}Nº de hojas:\\\\

\textbf{NOTAS:}
\begin{enumerate}
    \item\textbf{Todas las afirmaciones deben justificarse.}
    \item\textbf{Se valorará la calidad de la exposición de la respuesta, así como la precisión, concisión y rigor de esta.}
\end{enumerate}

\vspace{0.2cm}

\textbf{1.} \textit{(0.2 p)} En el modelo de Barabasi, ¿qué nodos tienen una mayor tasa de crecimiento? Justifica tu respuesta.\\\\
\textbf{2.} \textit{(0.2 p)} Considera dos redes aleatorias con el mismo número de nodos y probabilidades respectivas $p_1=0.002$ y $p_2=0.01$, ¿cuál de las dos tendrá mayor número de aristas? Justifica tu respuesta.\\\\
\textbf{3.} \textit{(0.2 p)} ¿Qué se entiende por componente gigante? Justifica tu respuesta.\\\\
\textbf{4.} \textit{(0.2 p)} ¿Por qué las redes libres de escala reciben este nombre? Justifica tu respuesta.\\\\
\textbf{5.} \textit{(0.6 p)} El taller ``Sobre ruedas'' tiene una línea de servicio destinada únicamente al recambio de ruedas. Si la llegada de los coches sigue una distribución de proceso Poisson a una tasa de 5 coches a la hora, siendo el tiempo de servicio exponencial y se sabe que el tiempo medio de espera en el sistema es de 48 minutos:
\begin{enumerate}
    \item ¿Cuál es el número medio de coches esperando a ser atendido en una línea de servicio?
    \item ¿Cuál es el tiempo medio que dedica el operario a cada coche?
    \item ¿Cuál es la probabilidad de que al llegar un coche se encuentre que hay otro esperando en la cola?
\end{enumerate}

Justifica tus respuestas.\\

\textbf{6.} Tenemos una fuente de memoria nula con alfabeto $\mathcal{A}=\{a,b,c,d,e,f,g\}$ y probabilidades $P(a)=0.2$, $P(b)=0.15$, $P(c)=0.1$, $P(d)=0.1$, $P(e)=0.2$, $P(f)=0.15$ y $P(g)=0.1$. Además, tenemos un alfabeto código $\mathcal{B}=\{a,b,c\}$ y la siguiente codificación $h$: $h(a)=a$, $h(b)=b$, $h(c)=c$, $h(d)=bb$, $h(e)=ab$, $h(f)=bab$ y $h(g) = bc$.

\begin{enumerate}
    \item\textit{(0.6 p)} ¿Es la codificación unívocamente decodificable? Justifica tu respuesta.
    \item\textit{(0.6 p)} ¿Cumple la desigualdad de Kraft? Justifica tu respuesta.
    \item\textit{(1 p)} Propón una codificación instantánea para la fuente empleando el método de Shannon.
    \item\textit{(0.6 p)} Calcula el rendimiento de la codificación propuesta.
    \item\textit{(1 p)} ¿Es la codificación propuesta unívocamente decodificable? Justifica tu respuesta.
\end{enumerate}

~\\
\textbf{¿Es la codificación unívocamente decodificable? Justifica tu respuesta.}\\

Este ejercicio se podía responder encontrando un contraejemplo (por ejemplo $h(bb) = h(d)$), o usando el algoritmo y viendo que $\mathcal{A}\cap f(\mathcal{A})\neq\emptyset$.

~\\
\textbf{¿Cumple la desigualdad de Kraft? Justifica tu respuesta.}\\
$$
\sum_{x\in f(\mathcal{A})} k^{-|x|} = 3^{-1} + 3^{-1} + 3^{-1} + 3^{-2} + 3^{-2} + 3^{-3} = \frac{3}{3} + \frac{3}{9} + \frac{1}{27} > 1	
$$

Por lo que la codificación no cumple la desigualdad de Kraft.

~\\
\textbf{Propón una codificación instantánea para la fuente empleando el método de Shannon.}\\

Aquí la base del logaritmo es 3 (porque el alfabeto código tiene 3 símbolos). Esto era crucial.\\

\begin{tabular}{|c|c|c|c|c|}
\hline
Símbolo & $p$ & $-\log_3(p)$ & $l$ & Código\\
\hline
$a$ & 0.2 & 1.46 & 2 & aa\\
$b$ & 0.15 & 1.73 & 2 & ab\\
$c$ & 0.1 & 2.0959 & 3 & bca\\
$d$ & 0.1 & 2.0959 & 3 & bcb\\
$e$ & 0.2 & 1.46 & 2 & ac\\
$f$ & 0.15 & 1.73 & 2 & bb\\
$g$ & 0.1 & 2.0959 & 3 & bcc\\
\hline
\end{tabular}

~\\
\textbf{Calcula el rendimiento de la codificación propuesta.}\\

$$
\eta = \frac{H_3(X)}{\mathcal{L}_f} = \frac{\sum_i -p_i\cdot\log_3\left(p_i\right)}{\sum_i p_i\cdot l_i}
$$

Y esos datos se sacaban de la tabla del ejercicio anterior. Si por algún motivo, alguien decide calcular $H_2(X)$, tiene que escalarlo a base 3 luego.


~\\
\textbf{¿Es la codificación propuesta unívocamente decodificable? Justifica tu respuesta.}\\

En este paso había que usar el algoritmo sí o sí (por eso vale más puntos que la otra pregunta que es similar). Para mi ejemplo\dots

\begin{enumerate}
    \item No es singular.
    \item $A = \emptyset \Rightarrow A\cap f(\mathcal{A})=\emptyset$
    \item $A' = \emptyset$
    \item No entro al bucle while.
    \item Termina el algoritmo. Se trata de una codificación unívocamente decodificable.
\end{enumerate}

\textbf{7.} La secuencia de estados climáticos de una ciudad puede entenderse como una fuente de información de Markov tomando como símbolos los estados climáticos: soleado ($S$), lluvioso ($L$) y nevado ($N$). Considera las siguientes probabilidades de transición entre estados:

\begin{center}
\begin{tabular}{l|r}

$P(S|S)$&$0.6$\\
$P(N|S)$&$0.3$\\
$P(L|S)$&$0.1$\\
$P(S|N)$&$0.4$\\
$P(N|N)$&$0.5$\\
$P(L|N)$&$0.1$\\
$P(S|L)$&$0.2$\\
$P(N|L)$&$0.3$\\
$P(L|L)$&$0.5$\\

\end{tabular}
\end{center}

\begin{enumerate}
    \item\textit{(0.4 p)} Dibuja el diagrama de estados de la fuente. ¿De qué orden es la fuente? Justifica tu respuesta.
    \item\textit{(0.4 p)} ¿Se trata de una fuente ergódica? Justifica tu respuesta.
    \item\textit{(1 p)} Calcula la entropía de la fuente.
    \item\textit{(1 p)} Calcula la entropía de la extensión de grado 5 de la fuente afín asociada a la fuente de Markov.
    \item\textit{(1 p)} En la fuente de Markov, ¿cuál es la información mutua entre los estados $S$ y $N$?
\end{enumerate}

~\\
\textbf{Dibuja el diagrama de estados de la fuente. ¿De qué orden es la fuente? Justifica tu respuesta.}\\

La fuente de Markov es de orden 1, porque las probabilidades de transición solamente dependen del estado inmediatamente anterior.\\

\textit{El diagrama consistía en 3 nodos (S, N y L) unidos por flechas, cuyos pesos eran las probabilidades de transición. Cada nodo estaba unido a los otros dos por flechas con probabilidades de transición, y a sí mismo.}

~\\
\textbf{¿Se trata de una fuente ergódica? Justifica tu respuesta.}\\

Sí, se trata de una fuente ergódica porque es posible pasar de un estado a otro en un número finito de pasos, sin caer en un estado absorbente.\\

~\\
\textbf{Calcula la entropía de la fuente.}\\

Calculamos primero las probabilidades en estado estacionario.

$$
\begin{cases}
P(S) = 0.6P(S) + 0.4P(N) + 0.2P(L)\\
P(N) = 0.3P(S) + 0.5P(N) + 0.3P(L)\\
P(S) + P(N) + P(L) = 1
\end{cases}
$$

En forma matricial:

$$
\begin{pmatrix}
0.4 & -0.4 & -0.2\\
-0.3 & 0.5 & -0.3\\
1 & 1 & 1
\end{pmatrix}
\begin{pmatrix}
P(S)\\
P(N)\\
P(L)
\end{pmatrix}
=
\begin{pmatrix}
0\\
0\\
1
\end{pmatrix}
$$

Los resultados son $P(S)=\frac{11}{24}\approx 0.4583$, $P(N)=\frac{3}{8}\approx 0.375$ y $P(L)=\frac{1}{6}\approx 0.17$.\\

Luego se aplica la definición de entropía de una fuente de información de Markov:

\begin{align*}
H(X) &= -\sum_{x\in\mathcal{X}}P(x)\sum_{y\in\mathcal{X}}P(y|x)\log\left(P(y|x)\right)\\
&= -P(S)\left(P(S|S)\log\left(P(S|S)\right)+P(N|S)\log\left(P(N|S)\right)+P(L|S)\log\left(P(L|S)\right)\right)\\
&-P(N)\left(P(S|N)\log\left(P(S|N)\right)+P(N|N)\log\left(P(N|N)\right)+P(L|N)\log\left(P(L|N)\right)\right)\\
&-P(L)\left(P(S|L)\log\left(P(S|L)\right)+P(N|L)\log\left(P(N|L)\right)+P(L|L)\log\left(P(L|L)\right)\right)\\
&\approx 1.35\text{ bits}
\end{align*}

~\\
\textbf{Calcula la entropía de la extensión de grado 5 de la fuente afín asociada a la fuente de Markov.}\\

La fuente afín es la fuente de memoria nula asociada a la fuente de markov, con símbolos $S$, $N$ y $L$; y probabilidades $P(S)=\frac{11}{24}$, $P(N)=\frac{3}{8}$ y $P(L)=\frac{1}{6}$. La entropía de la extensión de grado 5 de la fuente afín asociada a la fuente de Markov es:

$$
H(X^5) = 5H(X) = -5\sum_{x\in\mathcal{X}}P(x)\log\left(P(x)\right) = 5\left(\frac{11}{24}\log\left(\frac{24}{11}\right)+\frac{3}{8}\log\left(\frac{8}{3}\right)+\frac{1}{6}\log\left(6\right)\right)\approx 7.39\text{ bits}
$$

\pagebreak
\textbf{En la fuente de Markov, ¿cuál es la información mutua entre los estados $S$ y $N$?}\\

Esta era una pregunta trampa. Procedemos a calcular la información mutua, por ejemplo mediante la definición:

$$
I(S;N) = P(S,N)\log\left(\frac{P(S,N)}{P(N)P(S)}\right)
$$

Ahora bien, nótese que $P(S|N)P(N)\neq P(N|S)P(S)$ en nuestro caso, entonces nos toca ``romper'' esto en 2:

\begin{align*}
I(S;N) &= P(S|N)P(N)\log\left(\frac{P(S|N)P(N)}{P(N)P(S)}\right) + P(N|S)P(S)\log\left(\frac{P(N|S)P(S)}{P(N)P(S)}\right)\\
&= P(S|N)P(N)\log\left(\frac{P(S|N)}{P(S)}\right) + P(N|S)P(S)\log\left(\frac{P(N|S)}{P(S)}\right)\\
\end{align*}

Siendo esto

\begin{align*}
I(S;N) &= 0.4\cdot\frac{3}{8}\log\left(\frac{0.4}{\cdot\frac{11}{24}}\right) + 0.3\cdot\frac{11}{24}\log\left(\frac{0.3}{\frac{3}{8}}\right)\\
&\approx -0.07
\end{align*}

El resultado negativo llama la atención, puesto que carece de sentido en el contexto de la información mutua. Por lo tanto, había que comentar que la información mutua no puede ser negativa, y por tanto para nuestra fuente de Markov, la información mutua entre los estados $S$ y $N$ no tiene sentido.

~\\
\textbf{8.} \textit{(1 p)} La distribución log-normal es una distribución de probabilidad continua que aparece en una gran cantidad de fenómenos naturales, la cual está íntimamente relacionada con la distribución normal. La función de densidad de probabilidad de una variable aleatoria $X$ con distribución log-normal es:

$$
f(x)=\frac{1}{x\sigma\sqrt{2\pi}}e^{-\frac{(\ln\left(x\right)-\mu)^2}{2\sigma^2}}
$$

con $x\in\left(0,+\infty\right)$.\\

Adicionalmente, la función de densidad de probabilidad de una variable aleatoria $X$ con distribución normal es:

$$
f(x)=\frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}
$$

con $x\in\mathbb{R}$, y donde $\mu$ es la media y $\sigma^2$ es la varianza de la distribución.\\

Supongamos que tenemos una variable aleatoria cuya distribución es log-normal. Calcula su entropía.\\

La resolución de este ejercicio consistía en plantear la integral de la entropía de la variable aleatoria con distribución log-normal y resolverla (para ello emplearíamos el cambio de variable $t=\ln(x)$ y propiedades de la distribución normal). Resolución:

\begin{align*}
H(X)&= -\int_{0}^{+\infty}f(x)\log\left(f(x)\right)dx\\
&= -\int_{0}^{+\infty}\frac{1}{x\sigma\sqrt{2\pi}}e^{-\frac{(\ln\left(x\right)-\mu)^2}{2\sigma^2}}\log\left(\frac{1}{x\sigma\sqrt{2\pi}}e^{-\frac{(\ln\left(x\right)-\mu)^2}{2\sigma^2}}\right)dx\\
&= -\int_{0}^{+\infty}\frac{1}{x\sigma\sqrt{2\pi}}e^{-\frac{(\ln\left(x\right)-\mu)^2}{2\sigma^2}}\left(\log\left(\frac{1}{x\sigma\sqrt{2\pi}}\right)-\frac{(\ln\left(x\right)-\mu)^2}{2\sigma^2}\log\left(e\right)\right)dx\\
&= \frac{1}{\ln\left(2\right)}\int_{0}^{+\infty}\frac{\ln\left(x\right)}{x\sigma\sqrt{2\pi}}e^{-\frac{(\ln\left(x\right)-\mu)^2}{2\sigma^2}}dx\\
&+\log\left(\sigma\sqrt{2\pi}\right) \int_{0}^{+\infty}\frac{1}{x\sigma\sqrt{2\pi}}e^{-\frac{(\ln\left(x\right)-\mu)^2}{2\sigma^2}}dx\\
&+\log\left(e\right)\int_{0}^{+\infty}\frac{1}{x\sigma\sqrt{2\pi}}e^{-\frac{(\ln\left(x\right)-\mu)^2}{2\sigma^2}}\left(\frac{(\ln\left(x\right)-\mu)^2}{2\sigma^2}\right)dx\\
\begin{cases}t&=\ln(x)\\dt&=\frac{1}{x}dx\end{cases}\Rightarrow&= \frac{1}{\ln\left(2\right)}\int_{0}^{+\infty}\frac{t}{\sigma\sqrt{2\pi}}e^{-\frac{(t-\mu)^2}{2\sigma^2}}dt\\
&+\log\left(\sigma\sqrt{2\pi}\right)\\
&+\frac{\log\left(e\right)}{2\sigma^2}\int_{0}^{+\infty}\frac{(t-\mu)^2}{\sigma\sqrt{2\pi}}e^{-\frac{(t-\mu)^2}{2\sigma^2}}\\
&= \frac{\mu}{\ln\left(2\right)}+\log\left(\sigma\sqrt{2\pi}\right)+\frac{\log\left(e\right)}{2\sigma^2}\sigma^2\\
&= \mu\log\left(e\right)+\log\left(\sigma\sqrt{2\pi}\right)+\frac{\log\left(e\right)}{2}\\
&= \log\left(e^{\mu + \frac{1}{2}}\sigma\sqrt{2\pi}\right)\\
\end{align*}

La entropía de la fuente es $H(X)=\log\left(e^{\mu + \frac{1}{2}}\sigma\sqrt{2\pi}\right)$ bits.\\\\

\subsubsection*{\centering Formulario Teoría de Colas}

$$
\begin{matrix}
\rho=\frac{\lambda}{\mu} & L_S=\frac{\rho}{1-\rho}\\
W_S = W_Q + t_S & \\
L_S=\lambda.W_S& P(W_Q>t)=\rho e^{-\left(\mu-\lambda\right)t}\\
L_Q=\lambda.W_Q& P(W_S>t)=e^{-\left(\mu-\lambda\right)t}\\
&\\
\text{P}_n=\begin{cases}1-\rho & \text{ para } n=0\\\rho^n(1-\rho) & \text{ para } n\geq 0\\\end{cases}
\end{matrix}
$$

\subsubsection*{\centering Algoritmo para averiguar si una codificación es unívocamente decodificable}

\begin{align*}
    &\text{if }\left(\exists a,b\in\mathcal{A}: (a\neq b)\wedge(f(a)=f(b))\right)\text{then:}\\
    &\quad\quad\text{return \texttt{false}}\\
    &A=\left\{u: \left(\exists x,y\in f(\mathcal{A}):(x\neq y)\wedge (xu=y)\right)\right\}\\
    &\text{if}\left(A\cap f(A)\neq\emptyset\right)\text{ then:}\\
    &\quad\quad\text{return \texttt{false}}\\
    &A'=\emptyset\\
    &\text{while }A\neq\emptyset:\\
    &\quad\quad A'=A\cup A'\\
    &\quad\quad B = \left\{u:\left(\left(\exists x\in f(\mathcal{A}): xu\in A\right)\vee\left(\exists x\in A: xu\in f(\mathcal{A})\right)\right)\right\}\\
    &\quad\quad A = B - A'\\
    &\quad\quad\text{if }\left(A\cap f(\mathcal{A})\neq\emptyset\right)\text{ then:}\\
    &\quad\quad\quad\quad\text{return \texttt{false}}\\
    &\text{return \texttt{true}}\\
\end{align*}

% \subsubsection*{\centering Ejercicio para Laboratorio (poner en tarea, no en papel)}

% \textbf{1.} Genera una red aleatoria con 500 nodos y probabilidad $0.008$.
% \begin{enumerate}
%     \item\textit{(0.5 p)} Dibuja su distribución de grado y ajústala mediante una distribución adecuada (dibuja ambas distribuciones en la misma gráfica).
%     \item\textit{(0.2 p)} ¿Cuál es el grado máximo de la red que has generado?
%     \item\textit{(0.3 p)} ¿Hay algún \textit{hub}? Contesta a esta pregunta apoyándote en las características de tu red.
%     \item\textit{(0.4 p)} ¿En qué régimen se encuentra?
%     \item\textit{(0.3 p)} ¿Qué puedes decir del número de componentes conexas que existen en la red una vez determinado el régimen en que se encuentra? Compruébalo mediante algún comando de Python.
% \end{enumerate}

% \textbf{2.}
% \begin{enumerate}
%     \item\textit{(0.7 p)} ¿Las redes de Barabasi son independientes del tiempo? Comenta tu respuesta apoyándote en tres de este tipo de redes.
%     \item\textit{(0.5 p)} Representa en una misma gráfica la distribución de grado de las tres redes que has generado, así como la distribución de grados del modelo teórico que se ajusta a este tipo de redes.
%     \item\textit{(0.2 p)} ¿Cuáles son los grados máximos de las redes que has generado?
%     \item\textit{(0.3 p)} ¿Hay algún \textit{hub}? Contesta a esta pregunta apoyándote en las características de tu red.
% \end{enumerate}

% \textbf{3.} \textit{(0.6 p)} Justifica, apoyándote en la teoría, tus respuestas de los apartados 1.c) y 2.d) de las preguntas anteriores.

\end{document}